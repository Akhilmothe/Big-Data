Create a markdown in your Github account discussing the following topics:
- Big Data with example and types
- 6 ‘V’s of Big Data (define each)
- Phases of Big Data analysis (discuss each)
- Challenges in Big Data analysis (discuss each)
Use your OWN words. Mention the source(s) in APA style.
Submit the link to the file (make sure your instructor and grader can access it) and a PDF file.

## 1. Bigdata -
   
Certainly! Big data refers to large and complex sets of information that are not easy to process or handle and are difficult to analyse with traditional data processing tools and methods. It involves dealing with large amounts of data, constantly generated from various sources like social networks, including text, images, audio, and video data, IoT devices, healthcare records and devices, financial and online transactions, satellites, mobile devices, machines, customer interactions, and many more.

## Examples of big data -
Social media posts, messages, emails, satellite imagery, sensor data, weather data, medical records, financial and online transactions, health device data, stock market data, etc.

## Types of big data -

There are different kinds of big data, they are:
1. Structured Data - Data that is organised in the form of rows and columns like spreadsheet, excel and csv files.
2. Unstructured Data - Data which is not in a specific order like social media texts, audio, images and video content.
3. Semi-Structured Data - Data which is partially organised with labels like JSON, HTML, XML and Log files.
4. Batch Data - This data is nothing but collection of data in chunks or groups rather than processing individually or in real-time. Examples include student report cards, ATM transactions, online orders.
5. Streaming Data - Unlike batch data this data is continously generated, transmitted and processed over time. This data is very useful for organisations to make quick and organised desicions over chaninging conditions. Examples include social media data, sensor data, stock market data, traffic monitoring and many more.
6. Graph Data - This data is quite similar to Entity - Relationship kind of representation making connections between two things and making relationships. Examples like road networks, organisational hierarchies.
7. Spatio - temporal Data - This kind of data combines both location-based and time-based information to track and analyze objects in space and time. Examples includes weather data, environmental monitoring, traffic data and supply chain management etc.

## 2. 6 V’s of Big data -

The 6 ‘V’s of Big Data provide a framework for understanding and managing large and complex sets of information. 

![image](https://github.com/Akhilmothe/Big-Data/assets/114513479/153ea49a-ab15-4911-9a8d-ca939ba98a02)


 Sydle. (2023). Big Data: definition, importance, and types. Blog SYDLE. https://www.sydle.com/blog/big-data-	definition-importance-and-types-614b791388e600016afa7fc3


Volume : As its name suggests, volume refers to the ‘amount of data’ which is increasing day by day that is collected from a variety of sources and devices in a continuous manner.

Velocity : Big data velocity refers to the speed at which data is generated every day. A great example of big data that is generated with great velocity would be tweets in X app, Facebook and 	Instagram posts etc.

Variety : As there are many sources which are contributing to big data like structured, semi-structured and unstructured, the type of data that is generated from these sources is different. Hence, variety in big data refers to different forms of data.

Variability : Variability in big data refers to the number of inconsistencies in data because the data is changing constantly. For example, variability in stock market is like how stock prices can quickly go up and down, creating uncertainty for investors.

Veracity : Veracity is nothing but quality and accuracy of data. In order to get maximum benefit from the data we have to make sure that the information should be correct and reliable.

Value : Value in big data refers to finding valuable insights from the data which helps to make more informed decisions. 

## 3. Phases of Big Data analysis -

There are five phases involved in big data analysis. They are:

Data Acquisition and Recording: This means getting data from different places, like files, websites, or sensors. Saving them in a safe and organized way so it can be used for analysis at later stages. For example, when we use our credit card to purchase something system requires data like amount, data and location for transaction. This data is recorded in our statement so we can view them later and keep track of expenses.

Information extraction and cleaning: Information extraction is nothing but pulling out valuable information from larger dataset making it easier to understand. After you extract the information, you often need to clean it. This means removing the errors, mistakes, or inconsistencies in data is called data cleaning.

Data Integration, Aggregation, and Representation: Data integration and aggregation means bringing the data together from different sources and making it as whole and summarizing or grouping the data to see the big picture. Data representation is presenting same data in clear and understandable manner in the form of visualizations like graphs, charts, histograms etc.

Query Processing, Data Modelling, and Analysis: A query is a simple method to extract specific, valuable portions of data from a larger dataset. 	A data model is a conceptual framework that arranges data elements and defines how they interact with each other in an organized way. There are different types of data models, such as database models, entity-relationship models, semantic models etc. When it comes to big data analysis, using only SQL queries isn’t enough. This is because all the data is not stored in traditional SQL databases.

Interpretation : Interpretation involves the concept of understanding and explaining the meaning or significance of information, or results. Sometimes verifying the results requires additional data, using visual representations, such as charts or graphs, can be helpful in this process. It makes it easier to validate the results.

## 4. Challenges in Big Data analysis -

Challenge 1: Heterogeneity and Incompleteness.
Machine learning algorithms expect homogeneous data and cannot understand any kind of differences. We need to put more effort into making semi – structured data easy to represent, access, and analyze efficiently. Even after we’ve gone through the process of cleaning and fixing data, there’s a great possibility of missing information and inaccuracies.

Challenge 2: Scale
Scale of big data is a significant challenge. Since the volume of data is growing rapidly than available resources it is becoming harder to manage it. For example, think as you run a small online store, and you want to analyze customer data to improve business. You can easily use a spreadsheet for analysing it. After some years your business grows, more and more customers grows. Using a simple spreadsheet won’t work anymore we need specialized tools and infrastructure to manage and analyze such a large scale of data. This is the big issue in the world of big data. Cloud computing, distributed computing frameworks, data warehousing are possible solutions for this challenge.

Challenge 3: Timeliness
Timeliness in big data refers to the challenge of processing and analyzing data quickly. If you don’t analyze the data quickly you could miss the opportunities or even run into issues. To handle this challenge, there are special tools and technologies that help analyze data in real – time.

Challenge 4: Privacy
Privacy is a big concern in Big Data. This is because, in large datasets, it's possible to uncover new information, like personal details based on factors such as location and time. Keeping this data private is not just a technical problem but also involves societal considerations like ethics and public perception. It's a challenge to protect people's data while still benefiting from the insights Big Data can provide through responsible data sharing. 

Challenge 5: Human Collaboration
Human collaboration can be a challenge in Big Data. Handling and making sense of large and complex datasets often require the expertise and collaboration of people from different domains, such as data scientists, engineers, domain experts, and business analysts. This means that humans play a crucial role in understanding and interpreting the data. In practice, it's often a team effort, where different experts work together. However, dealing with disagreements, uncertainties, and mistakes from different sources should be done effectively.


   

